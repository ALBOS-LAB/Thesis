{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Tables\n",
    "codes to clean data received from QGIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pyomo.environ as pyo \n",
    "import numpy  as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "import seaborn as sns\n",
    "\n",
    "Data_path_GDrive = \"G:/Il mio Drive/Thesis_Large_Files/Working Table\"\n",
    "Data_path_Production = \"C:/Users/Nik/Documents/GitHub/Thesis/CSV/Production\"\n",
    "Data_path_Consumption = \"C:/Users/Nik/Documents/GitHub/Thesis/CSV/Consumption\"\n",
    "Data_path_Users = \"C:/Users/Nik/Documents/GitHub/Thesis/CSV/Users\"\n",
    "results_dir = 'C:/Users/Nik/Documents/GitHub/Thesis/Results/Results_Binary/Final Results'\n",
    "results_dir = os.path.join(results_dir, datetime.now().strftime('%Y-%m-%d_%H-%M-%S') + '_BinaryProduction')\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "plot_dir = os.path.join(results_dir,'Plot')\n",
    "yearly_plot_dir = os.path.join(plot_dir,'Yearly_Plot')\n",
    "weekly_plot_dir = os.path.join(plot_dir,'Weekly_Plot')\n",
    "os.makedirs(yearly_plot_dir, exist_ok=True)\n",
    "os.makedirs(weekly_plot_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a uniform datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DataFrames\n",
    "DF_name = \"2019_ProductionEast.csv\"\n",
    "DF_path = os.path.join(Data_path_Production, DF_name)\n",
    "DF_East = pd.read_csv(DF_path, sep=\",\", on_bad_lines='skip', index_col=0, header=0, parse_dates=[0])\n",
    "\n",
    "DF_name = \"2019_ProductionSouth.csv\"\n",
    "DF_path = os.path.join(Data_path_Production, DF_name)\n",
    "DF_South = pd.read_csv(DF_path, sep=\",\", on_bad_lines='skip', index_col=0, header=0, parse_dates=[0])\n",
    "\n",
    "DF_name = \"2019_ProductionWest.csv\"\n",
    "DF_path = os.path.join(Data_path_Production, DF_name)\n",
    "DF_West = pd.read_csv(DF_path, sep=\",\", on_bad_lines='skip', index_col=0, header=0, parse_dates=[0])\n",
    "\n",
    "DF_name = \"2019_ConsumptionCurve.csv\"\n",
    "DF_path = os.path.join(Data_path_Consumption, DF_name)\n",
    "DF_Consumption = pd.read_csv(DF_path, sep=\",\", on_bad_lines='skip', index_col=0, header=0, parse_dates=[0])\n",
    "\n",
    "DF_name = \"Thesis_User_Residential.csv\"\n",
    "DF_path = os.path.join(Data_path_Users, DF_name)\n",
    "DF_m2_Residential = pd.read_csv(DF_path,sep=\",\",on_bad_lines='skip', header=0, parse_dates=[0])\n",
    "\n",
    "DF_name = \"Thesis_User_Industrial.csv\"\n",
    "DF_path = os.path.join(Data_path_Users, DF_name)\n",
    "DF_m2_Industrial = pd.read_csv(DF_path,sep=\",\",on_bad_lines='skip', header=0, parse_dates=[0])\n",
    "\n",
    "DF_name = \"Thesis_User_Commercial.csv\"\n",
    "DF_path = os.path.join(Data_path_Users, DF_name)\n",
    "DF_m2_Commercial = pd.read_csv(DF_path,sep=\",\",on_bad_lines='skip', header=0, parse_dates=[0])\n",
    "\n",
    "# List of your dataframes\n",
    "dataframes = [DF_South, DF_East, DF_West]\n",
    "\n",
    "# Function to change the datetime index format\n",
    "def change_datetime_index_format(df):\n",
    "    df.index = df.index.astype(str)\n",
    "    df.index = pd.to_datetime(df.index, format='%Y%m%d:%H%M')\n",
    "    df.index = df.index.strftime('%Y%m%d %H:%M')\n",
    "    df.index = pd.to_datetime(df.index, format='%Y%m%d %H:%M')\n",
    "    return df\n",
    "\n",
    "# Apply the function to each dataframe\n",
    "dataframes = [change_datetime_index_format(df) for df in dataframes]\n",
    "\n",
    "# Unpack the dataframes back to their original names if needed\n",
    "DF_South, DF_East, DF_West = dataframes\n",
    "\n",
    "# Extract the G(i) column from each DataFrame and rename it\n",
    "DF_South_Gi = DF_South[['G(i)']].rename(columns={'G(i)': 'G(i)_South'})\n",
    "DF_East_Gi = DF_East[['G(i)']].rename(columns={'G(i)': 'G(i)_East'})\n",
    "DF_West_Gi = DF_West[['G(i)']].rename(columns={'G(i)': 'G(i)_West'})\n",
    "\n",
    "# Combine the columns into a new DataFrame\n",
    "DF_Production = pd.concat([DF_South_Gi, DF_East_Gi, DF_West_Gi], axis=1)\n",
    "\n",
    "# Define paths to save the files\n",
    "DF_Production_name_csv = \"2019_Production.csv\"\n",
    "DF_Production_name_xlsx = \"2019_Production.xlsx\"\n",
    "DF_path_csv = os.path.join(Data_path_Production, DF_Production_name_csv)\n",
    "DF_path_xlsx = os.path.join(Data_path_Production, DF_Production_name_xlsx)\n",
    "\n",
    "# Save the new DataFrame to CSV and Excel\n",
    "DF_Production.to_csv(DF_path_csv, index=True)\n",
    "DF_Production.to_excel(DF_path_xlsx, index=True)\n",
    "\n",
    "# Save the updated DF_Consumption back to CSV with the modified index\n",
    "DF_Consumption_name_csv = \"2019_ConsumptionCurve.csv\"\n",
    "DF_path_consumption_csv = os.path.join(Data_path_Consumption, DF_Consumption_name_csv)\n",
    "DF_Consumption.to_csv(DF_path_consumption_csv, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjust date format to a convenient one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def adjust_date_format_for_datetime_index(folder_path):\n",
    "    \"\"\"\n",
    "    Adjusts the date format of CSV files in the given folder path that have a datetime index,\n",
    "    by rounding the index to the nearest hour.\n",
    "\n",
    "    Parameters:\n",
    "    folder_path (str): The path to the folder containing CSV files.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Loop through all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file is a CSV file\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            # Read the CSV file\n",
    "            try:\n",
    "                # Load the first few rows to check the index\n",
    "                df = pd.read_csv(file_path, index_col=0)\n",
    "                \n",
    "                # Check if the index is already in datetime format or can be converted\n",
    "                try:\n",
    "                    # Try to convert the index to datetime format\n",
    "                    datetime_index = pd.to_datetime(df.index, errors='raise')\n",
    "                    \n",
    "                    # If successful, set the converted index and proceed to round it\n",
    "                    df.index = datetime_index\n",
    "                    \n",
    "                    # Round the index to the nearest hour\n",
    "                    df.index = df.index.round('H')\n",
    "\n",
    "                    # Save the adjusted DataFrame back to the original file\n",
    "                    df.to_csv(file_path)\n",
    "\n",
    "                    print(f\"Processed and saved: {file_path}\")\n",
    "\n",
    "                except Exception as datetime_error:\n",
    "                    # If the index cannot be converted to datetime, skip the file\n",
    "                    print(f\"Skipped {file_path}: Index is not a valid datetime format. Error: {datetime_error}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {file_path}: {e}\")\n",
    "\n",
    "# Run the function with your specified directory\n",
    "folder_directory = r'C:\\Users\\Nik\\Documents\\GitHub\\Thesis\\CSV\\Production'\n",
    "adjust_date_format_for_datetime_index(folder_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the eligible users to for the PV installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from paper for available roof production Area\n",
    "DoublePitched = 0.5 * 0.6 * 0.85 * 0.7 * 0.5\n",
    "Industrial = 0.8 * 0.9 * 1 * 0.45 * 1 * 0.45 * 1\n",
    "Flat = 1 * 0.6 * 0.9 * 0.45 * 0.5\n",
    "costheta_Res = np.cos(20 * np.pi / 180)\n",
    "costheta_Ind = np.cos(30 * np.pi / 180)\n",
    "costheta_Com = np.cos(0 * np.pi / 180)\n",
    "CoefRes = (DoublePitched * 0.95 + Flat * 0.05) / costheta_Res\n",
    "CoefInd = Industrial / costheta_Ind\n",
    "CoefCom = (Flat * 0.5 + DoublePitched * 0.5) / costheta_Com\n",
    "Limit = 2\n",
    "nPVLimit = 1.63 * Limit\n",
    "\n",
    "#Change Area from string to float\n",
    "DF_m2_Residential['Area'] = DF_m2_Residential['Area'].astype(str)\n",
    "DF_m2_Industrial['Area'] = DF_m2_Industrial['Area'].astype(str)\n",
    "DF_m2_Commercial['Area'] = DF_m2_Commercial['Area'].astype(str)\n",
    "\n",
    "DF_m2_Residential['Area'] = DF_m2_Residential['Area'].str.replace(',', '.')\n",
    "DF_m2_Industrial['Area'] = DF_m2_Industrial['Area'].str.replace(',', '.')\n",
    "DF_m2_Commercial['Area'] = DF_m2_Commercial['Area'].str.replace(',', '.')\n",
    "\n",
    "DF_m2_Residential['Area'] = DF_m2_Residential['Area'].astype(float)\n",
    "DF_m2_Industrial['Area'] = DF_m2_Industrial['Area'].astype(float)\n",
    "DF_m2_Commercial['Area'] = DF_m2_Commercial['Area'].astype(float)\n",
    "DF_m2_Residential['Area'] = DF_m2_Residential['Area'] * CoefRes\n",
    "DF_m2_Industrial['Area'] = DF_m2_Industrial['Area'] * CoefInd\n",
    "DF_m2_Commercial['Area'] = DF_m2_Commercial['Area'] * CoefCom\n",
    "\n",
    "# Delete rows where 'Area' column values are less than nPVLimit\n",
    "DF_m2_Residential = DF_m2_Residential[DF_m2_Residential['Area'] >= nPVLimit]\n",
    "DF_m2_Industrial = DF_m2_Industrial[DF_m2_Industrial['Area'] >= nPVLimit]\n",
    "DF_m2_Commercial = DF_m2_Commercial[DF_m2_Commercial['Area'] >= nPVLimit]\n",
    "\n",
    "# Reset the index\n",
    "DF_m2_Residential = DF_m2_Residential.reset_index(drop=True)\n",
    "DF_m2_Industrial = DF_m2_Industrial.reset_index(drop=True)\n",
    "DF_m2_Commercial = DF_m2_Commercial.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate randomly the exposure coefficients of the users' houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate random alpha values with two decimal places\n",
    "def generate_alpha_columns(df):\n",
    "    def round_and_adjust(row):\n",
    "        row = np.round(row, 2)\n",
    "        row_diff = 1.0 - row.sum()\n",
    "        for i in range(len(row)):\n",
    "            if row_diff == 0:\n",
    "                break\n",
    "            adjustment = np.round(min(row_diff, 0.01 if row_diff > 0 else -0.01), 2)\n",
    "            if (row[i] + adjustment >= 0) and (row[i] + adjustment <= 1):\n",
    "                row[i] += adjustment\n",
    "                row_diff -= adjustment\n",
    "        return row\n",
    "\n",
    "    # Generate random values\n",
    "    random_values = np.random.rand(len(df), 3)\n",
    "    # Normalize the values to ensure they sum to 1\n",
    "    random_values /= random_values.sum(axis=1)[:, None]\n",
    "    # Round values to two decimal places and adjust to ensure they sum to 1\n",
    "    adjusted_values = np.apply_along_axis(round_and_adjust, 1, random_values)\n",
    "    # Create new columns\n",
    "    df['S_coeff'] = adjusted_values[:, 0]\n",
    "    df['W_coeff'] = adjusted_values[:, 1]\n",
    "    df['E_coeff'] = adjusted_values[:, 2]\n",
    "    return df\n",
    "\n",
    "# Assuming DF_m2_Residential, DF_m2_Industrial, DF_m2_Commercial are already defined\n",
    "dataframes = [DF_m2_Residential, DF_m2_Industrial, DF_m2_Commercial]\n",
    "\n",
    "# Apply the function to each dataframe\n",
    "dataframes = [generate_alpha_columns(df) for df in dataframes]\n",
    "\n",
    "# Unpack the dataframes back to their original names if needed\n",
    "DF_m2_Residential, DF_m2_Industrial, DF_m2_Commercial = dataframes\n",
    "\n",
    "#Save\n",
    "DF_m2_Residential.to_csv(os.path.join(results_dir, 'DF_m2_Residential.csv'), index=False)\n",
    "DF_m2_Industrial.to_csv(os.path.join(results_dir, 'DF_m2_Industrial.csv'), index=False)\n",
    "DF_m2_Commercial.to_csv(os.path.join(results_dir, 'DF_m2_Commercial.csv'), index=False)\n",
    "\n",
    "# Calculate the production for each residential user\n",
    "Residential_user_production = (DF_Production['G(i)_South'].values[:, None] * DF_m2_Residential['S_coeff'].values +\n",
    "                               DF_Production['G(i)_East'].values[:, None] * DF_m2_Residential['E_coeff'].values +\n",
    "                               DF_Production['G(i)_West'].values[:, None] * DF_m2_Residential['W_coeff'].values)\n",
    "\n",
    "# Create a DataFrame for residential user production\n",
    "Residential_user_production_df = pd.DataFrame(Residential_user_production, index=DF_Production.index, columns=DF_m2_Residential.index)\n",
    "\n",
    "# Calculate the production for each industrial user\n",
    "Industrial_user_production = (DF_Production['G(i)_South'].values[:, None] * DF_m2_Industrial['S_coeff'].values +\n",
    "                              DF_Production['G(i)_East'].values[:, None] * DF_m2_Industrial['E_coeff'].values +\n",
    "                              DF_Production['G(i)_West'].values[:, None] * DF_m2_Industrial['W_coeff'].values)\n",
    "\n",
    "# Create a DataFrame for industrial user production\n",
    "Industrial_user_production_df = pd.DataFrame(Industrial_user_production, index=DF_Production.index, columns=DF_m2_Industrial.index)\n",
    "\n",
    "# Calculate the production for each commercial user\n",
    "Commercial_user_production = (DF_Production['G(i)_South'].values[:, None] * DF_m2_Commercial['S_coeff'].values +\n",
    "                              DF_Production['G(i)_East'].values[:, None] * DF_m2_Commercial['E_coeff'].values +\n",
    "                              DF_Production['G(i)_West'].values[:, None] * DF_m2_Commercial['W_coeff'].values)\n",
    "\n",
    "# Create a DataFrame for commercial user production\n",
    "Commercial_user_production_df = pd.DataFrame(Commercial_user_production, index=DF_Production.index, columns=DF_m2_Commercial.index)\n",
    "\n",
    "# Save the user production DataFrames to CSV files\n",
    "Residential_user_production_df.to_csv(os.path.join(results_dir, 'Residential_user_production.csv'))\n",
    "Industrial_user_production_df.to_csv(os.path.join(results_dir, 'Industrial_user_production.csv'))\n",
    "Commercial_user_production_df.to_csv(os.path.join(results_dir, 'Commercial_user_production.csv'))\n",
    "\n",
    "# Check the result (optional)\n",
    "print(Residential_user_production_df.head())\n",
    "print(Industrial_user_production_df.head())\n",
    "print(Commercial_user_production_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Ensure time column is set as the index and converted to datetime format\n",
    "DF_Production['time'] = pd.to_datetime(DF_Production['time'])\n",
    "DF_Production.set_index('time', inplace=True)'''\n",
    "\n",
    "# Calculate the production for each residential user\n",
    "Residential_user_production = (\n",
    "    DF_Production['G(i)_South'].values[:, None] * DF_m2_Residential['S_coeff'].values +\n",
    "    DF_Production['G(i)_East'].values[:, None] * DF_m2_Residential['E_coeff'].values +\n",
    "    DF_Production['G(i)_West'].values[:, None] * DF_m2_Residential['W_coeff'].values\n",
    ")\n",
    "\n",
    "# Create a DataFrame for residential user production\n",
    "Residential_user_production_df = pd.DataFrame(\n",
    "    Residential_user_production,\n",
    "    index=DF_Production.index,\n",
    "    columns=[f'Residential_User_{full_id}_Coeff' for full_id in DF_m2_Residential['full_id']]\n",
    ")\n",
    "\n",
    "# Similarly, create DataFrames for industrial and commercial user production\n",
    "Industrial_user_production = (\n",
    "    DF_Production['G(i)_South'].values[:, None] * DF_m2_Industrial['S_coeff'].values +\n",
    "    DF_Production['G(i)_East'].values[:, None] * DF_m2_Industrial['E_coeff'].values +\n",
    "    DF_Production['G(i)_West'].values[:, None] * DF_m2_Industrial['W_coeff'].values\n",
    ")\n",
    "Industrial_user_production_df = pd.DataFrame(\n",
    "    Industrial_user_production,\n",
    "    index=DF_Production.index,\n",
    "    columns=[f'Industrial_User_{full_id}_Coeff' for full_id in DF_m2_Industrial['full_id']]\n",
    ")\n",
    "\n",
    "Commercial_user_production = (\n",
    "    DF_Production['G(i)_South'].values[:, None] * DF_m2_Commercial['S_coeff'].values +\n",
    "    DF_Production['G(i)_East'].values[:, None] * DF_m2_Commercial['E_coeff'].values +\n",
    "    DF_Production['G(i)_West'].values[:, None] * DF_m2_Commercial['W_coeff'].values\n",
    ")\n",
    "Commercial_user_production_df = pd.DataFrame(\n",
    "    Commercial_user_production,\n",
    "    index=DF_Production.index,\n",
    "    columns=[f'Commercial_User_{full_id}_Coeff' for full_id in DF_m2_Commercial['full_id']]\n",
    ")\n",
    "\n",
    "# Save the user production DataFrames to CSV files\n",
    "Residential_user_production_df.to_csv(os.path.join(Data_path_GDrive, 'Residential_user_production.csv'))\n",
    "Industrial_user_production_df.to_csv(os.path.join(Data_path_GDrive, 'Industrial_user_production.csv'))\n",
    "Commercial_user_production_df.to_csv(os.path.join(Data_path_GDrive, 'Commercial_user_production.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to rename columns by removing '*_User_' and '_Mixed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved as C:\\Users\\Nik\\Desktop\\Backup thesis\\Consumption\\Residential_Consumption_FromBackup.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def rename_columns_and_save(input_file_path, output_file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(input_file_path)\n",
    "    \n",
    "    # Rename the columns\n",
    "    df.columns = [col.replace('Residential_User_', '').replace('_Mixed', '') if 'Residential_User_' in col else col for col in df.columns]\n",
    "    \n",
    "    # Save the modified DataFrame to a new CSV file\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(f\"File saved as {output_file_path}\")\n",
    "\n",
    "\n",
    "input_file_path = r'C:\\Users\\Nik\\Desktop\\Backup thesis\\Consumption\\Residential_Consumption_Optimized_From_Switched_2.csv'\n",
    "output_file_path = r'C:\\Users\\Nik\\Desktop\\Backup thesis\\Consumption\\Residential_Consumption_FromBackup.csv'\n",
    "rename_columns_and_save(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to modify the time in the datetime index to set minutes to 00\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_time_to_hour(dataframe):\n",
    "    # Convert the index to datetime format (if it's not already) and floor the minute to 'H' (hours)\n",
    "    dataframe.index = pd.to_datetime(dataframe.index).floor('H')\n",
    "    return dataframe\n",
    "\n",
    "# Example usage:\n",
    "df = pd.read_csv(r'C:\\Users\\Nik\\Desktop\\Backup thesis\\Consumption\\Residential_Consumption_FromBackup.csv', index_col=0)  # Ensure you load your file with the first column as index\n",
    "df_modified = modify_time_to_hour(df)\n",
    "\n",
    "# Save the modified dataframe to a new CSV file\n",
    "df_modified.to_csv(r'C:\\Users\\Nik\\Desktop\\Backup thesis\\Consumption\\Residential_Consumption_FromBackupFinal.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete columns to decrease the size of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random columns and corresponding rows have been successfully deleted.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load the data files\n",
    "residential_consumption_path = r'C:\\Users\\Nik\\Desktop\\Backup thesis\\Consumption\\Residential_Consumption_FromBackupFinal.csv'\n",
    "residential_production_path = r'G:\\Il mio Drive\\Thesis_Large_Files\\Working Table\\Residential_user_production.csv'\n",
    "df_m2_residential_path = r'C:\\Users\\Nik\\Documents\\GitHub\\Thesis\\CSV\\Working Tables\\DF_m2_Residential.csv'\n",
    "\n",
    "# Load the data into DataFrames\n",
    "residential_consumption_df = pd.read_csv(residential_consumption_path)\n",
    "residential_production_df = pd.read_csv(residential_production_path)\n",
    "df_m2_residential_df = pd.read_csv(df_m2_residential_path)\n",
    "\n",
    "# Select 4000 random columns from residential_consumption_df\n",
    "random_columns = random.sample(list(residential_consumption_df.columns), 4000)\n",
    "\n",
    "# Find the full_ids associated with these columns\n",
    "full_ids_to_delete = set(random_columns).intersection(df_m2_residential_df['full_id'])\n",
    "\n",
    "# Drop the selected columns in both consumption and production data\n",
    "residential_consumption_df.drop(columns=full_ids_to_delete, inplace=True, errors='ignore')\n",
    "residential_production_df.drop(columns=full_ids_to_delete, inplace=True, errors='ignore')\n",
    "\n",
    "# Remove rows in df_m2_residential_df where full_id is in the list of columns to delete\n",
    "df_m2_residential_df = df_m2_residential_df[~df_m2_residential_df['full_id'].isin(full_ids_to_delete)]\n",
    "\n",
    "# Save the modified dataframes back to CSV\n",
    "residential_consumption_df.to_csv(r'C:\\Users\\Nik\\Desktop\\Backup thesis\\Consumption\\ResidentialConsumption-4000\\Modified_Residential_Consumption.csv', index=False)\n",
    "residential_production_df.to_csv(r'C:\\Users\\Nik\\Desktop\\Backup thesis\\Consumption\\ResidentialConsumption-4000\\Modified_Residential_Production.csv', index=False)\n",
    "df_m2_residential_df.to_csv(r'C:\\Users\\Nik\\Desktop\\Backup thesis\\Consumption\\ResidentialConsumption-4000\\Modified_DF_m2_Residential.csv', index=False)\n",
    "\n",
    "print(\"Random columns and corresponding rows have been successfully deleted.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
